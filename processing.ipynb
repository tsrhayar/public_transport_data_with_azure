{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd00173c-235d-403b-993f-e1b6b8e88eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'public_transport_data_month_1' déjà traité.\n'public_transport_data_month_2' déjà traité.\n'public_transport_data_month_3' déjà traité.\n'public_transport_data_month_4' déjà traité.\n'public_transport_data_month_5' déjà traité.\nTous les fichiers sont traité\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, when, lpad, concat\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_timestamp, unix_timestamp, col, when, hour, avg\n",
    "\n",
    "def process_public_transport_data(input_file):\n",
    "    # Création de la session Spark\n",
    "    spark = SparkSession.builder.appName(\"DataLakeIntegration\").getOrCreate()\n",
    "\n",
    "    # Configuration de l'accès au stockage Azure\n",
    "    spark.conf.set(f\"fs.azure.account.key.tahasrhstorageaccount.dfs.core.windows.net\",\n",
    "                   \"VSOLoQmP3NugMwrGCd/C7ZHaz+7/e6bjB4jI1rNp1GaWeGIS9MvvzYChfyR1xXhJWNaC0qQLICnQ+ASttWp/+w==\")\n",
    "\n",
    "    # Emplacement du fichier source dans Azure Data Lake Storage Gen2\n",
    "    file_location = f\"abfss://public-transport-data@tahasrhstorageaccount.dfs.core.windows.net/raw/{input_file}\"\n",
    "\n",
    "    # Lecture du fichier CSV dans un DataFrame\n",
    "    df = spark.read.format(\"csv\").option(\"inferSchema\", \"True\").option(\"header\", \"True\").option(\"delimiter\", \",\").load(\n",
    "        file_location)\n",
    "\n",
    "    # Transformation de l'heure d'arrivée\n",
    "    def transform_arrival_time(time_col):\n",
    "        hours = expr(\"CAST(SUBSTRING_INDEX({}, ':', 1) AS INT)\".format(time_col))\n",
    "        minutes = expr(\"SUBSTRING({}, -2, 2)\".format(time_col))\n",
    "\n",
    "        transformed_hours = when(hours >= 24, hours - 24).otherwise(hours)\n",
    "        transformed_hours_padded = lpad(transformed_hours, 2, '0')\n",
    "\n",
    "        return concat(transformed_hours_padded, expr(\"':'\"), minutes)\n",
    "\n",
    "    # Application de la transformation à la colonne ArrivalTime\n",
    "    df = df.withColumn(\"ArrivalTime\", transform_arrival_time(\"ArrivalTime\"))\n",
    "\n",
    "    # Transformation pour extraire l'année, le mois, le jour et le jour de la semaine\n",
    "    df = df.withColumn(\"Year\", F.year(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Month\", F.month(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Day\", F.dayofmonth(df[\"Date\"]))\n",
    "    df = df.withColumn(\"DayOfWeek\", F.date_format(df[\"Date\"], \"EEEE\"))\n",
    "\n",
    "    # Ajustement des heures d'arrivée si elles sont antérieures à l'heure de départ\n",
    "    df = df.withColumn(\n",
    "        \"ArrivalTime\",\n",
    "        F.when(df[\"ArrivalTime\"] < df[\"DepartureTime\"], df[\"ArrivalTime\"] + F.expr(\"INTERVAL 1 DAY\"))\n",
    "        .otherwise(df[\"ArrivalTime\"])\n",
    "    )\n",
    "\n",
    "    # Conversion de DepartureTime et ArrivalTime en timestamp\n",
    "    df = df.withColumn(\"DepartureTime\", to_timestamp(df[\"DepartureTime\"]))\n",
    "    df = df.withColumn(\"ArrivalTime\", to_timestamp(df[\"ArrivalTime\"]))\n",
    "\n",
    "    # Calcul de la durée du voyage en heures\n",
    "    df = df.withColumn(\"TripDurationHours\", (unix_timestamp(\"ArrivalTime\") - unix_timestamp(\"DepartureTime\")) / 3600)\n",
    "\n",
    "    # Catégorisation de la durée du voyage en \"Pas de Retard\", \"Retard Court\", \"Retard Moyen\" ou \"Long Retard\"\n",
    "    df = df.withColumn(\n",
    "        \"DelayCategory\",\n",
    "        when(col(\"TripDurationHours\") <= 0, \"Pas de Retard\")\n",
    "        .when((col(\"TripDurationHours\") > 0) & (col(\"TripDurationHours\") <= 0.1667), \"Retard Court\")\n",
    "        .when((col(\"TripDurationHours\") > 0.1667) & (col(\"TripDurationHours\") <= 0.3333), \"Retard Moyen\")\n",
    "        .otherwise(\"Long Retard\")\n",
    "    )\n",
    "\n",
    "    # Extraction de l'heure de départ\n",
    "    df = df.withColumn(\"DepartureHour\", hour(\"DepartureTime\"))\n",
    "\n",
    "    # Calcul de la moyenne des passagers par heure de départ\n",
    "    passenger_avg_by_hour = df.groupBy(\"DepartureHour\").agg(avg(\"Passengers\").alias(\"AvgPassengers\"))\n",
    "\n",
    "    # Définition du seuil pour les heures de pointe\n",
    "    pointe_threshold = 50  # Par exemple, supposons que toute heure avec une moyenne de passagers > 50 est considérée comme une heure de pointe\n",
    "\n",
    "    # Catégorisation des heures en \"Heure de Pointe\" ou \"Heure Hors Pointe\"\n",
    "    passenger_avg_by_hour = passenger_avg_by_hour.withColumn(\n",
    "        \"HourCategory\",\n",
    "        when(col(\"AvgPassengers\") > pointe_threshold, \"Heure de Pointe\").otherwise(\"Heure Hors Pointe\")\n",
    "    )\n",
    "\n",
    "    # Spécification de l'emplacement de destination dans Azure Data Lake Storage Gen2\n",
    "    output_path = f\"abfss://public-transport-data@tahasrhstorageaccount.dfs.core.windows.net/processed/\"\n",
    "\n",
    "    # Écriture du DataFrame dans Azure Data Lake Storage Gen2 avec le nom de fichier spécifié\n",
    "    df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"True\").option(\"path\", output_path + input_file).save()\n",
    "\n",
    "    # Affichage du DataFrame\n",
    "    # display(df)\n",
    "\n",
    "#### --------------- ####\n",
    "\n",
    "# Spécifiez le chemin du \"répertoire brut\" dans Azure Data Lake Storage Gen2\n",
    "raw_directory = \"abfss://public-transport-data@tahasrhstorageaccount.dfs.core.windows.net/raw/\"\n",
    "\n",
    "# Spécifiez le chemin du \"répertoire traité\" où vous souhaitez vérifier les noms de répertoires\n",
    "processed_directory = \"abfss://public-transport-data@tahasrhstorageaccount.dfs.core.windows.net/processed/\"\n",
    "\n",
    "# Listez tous les fichiers dans le \"répertoire brut\"\n",
    "file_list = [fichier.name for fichier in dbutils.fs.ls(raw_directory)]\n",
    "\n",
    "# Listez tous les répertoires dans le \"répertoire traité\"\n",
    "processed_dirs = [dossier.name for dossier in dbutils.fs.ls(processed_directory) if dossier.isDir]\n",
    "\n",
    "# Vérifiez si les noms de fichiers existent en tant que noms de répertoires dans le \"répertoire traité\"\n",
    "counter = 0\n",
    "total_files = len(file_list)\n",
    "\n",
    "for idx, nom_fichier in enumerate(file_list):\n",
    "    if nom_fichier + \"/\" in processed_dirs:\n",
    "        print(f\"'{nom_fichier}' déjà traité.\")\n",
    "    else:\n",
    "        # Traitez les données de transport public (remplacez ceci par votre logique de traitement réelle)\n",
    "        process_public_transport_data(nom_fichier)\n",
    "        print(f\"'{nom_fichier}' traité avec succès.\")\n",
    "        counter += 1\n",
    "\n",
    "    if counter == 2 or idx == total_files - 1:\n",
    "        if idx == total_files - 1 and counter == 0:\n",
    "            print(\"Tous les fichiers sont traité\")\n",
    "        else:\n",
    "            print(\"Limite de traitement atteinte. Sortie de la boucle.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7405b1d6-2d68-4e26-aa83-5f00cad9f402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-09-27 14_29_53",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
